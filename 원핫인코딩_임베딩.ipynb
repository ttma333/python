{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNYaFdgkouisL4QFzOruG1b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttma333/python/blob/main/%EC%9B%90%ED%95%AB%EC%9D%B8%EC%BD%94%EB%94%A9_%EC%9E%84%EB%B2%A0%EB%94%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BLypXQfrDCS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8adc73b4-5684-42a9-fd70-7aa81c4a81c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/딥러닝 수업\n"
          ]
        }
      ],
      "source": [
        "cd/content/drive/MyDrive/딥러닝 수업"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy로 원핫인코딩\n",
        "import numpy as np\n",
        "\n",
        "samples = ['나는 오늘도 어제처럼 자연어처리 공부를 한다', '한파이와 함께라면 자연어처리 공부를 쉽게 한다']\n",
        "token_index = {}\n",
        "# print('numpy 확인')\n",
        "for sample in samples:\n",
        "    for word in sample.split(): # split로 띄어쓰기 기준으로 토큰을 나눈다\n",
        "        # print(word)\n",
        "        if word not in token_index:\n",
        "            token_index[word] = len(token_index) + 1     # 0 인덱스는 사용하지 않는다.\n",
        "# print(token_index)    # //{'나는': 1, '오늘도': 2, '어제처럼': 3, '자연어처리': 4, '공부를': 5, '한다': 6, '한파이와': 7, '함께라면': 8, '쉽게': 9}\n",
        "\n",
        "max_length = 10 # 각 샘플의 단어는 max_length까지만 허용한다. 즉, 여기서는 한 문장에 10개의 단어까지만 허용 할 수 있다는 이야기다.\n",
        "results = np.zeros(shape=(len(samples),\n",
        "                            max_length,\n",
        "                            max(token_index.values()) + 1))\n",
        "# print(token_index.values())      # dict_values([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "print(results)                   # 결과를 저장할 배열을 만든거다.\n",
        "\n",
        "for i, sample in enumerate(samples):     # enumerate는 인덱스부분을 i에서 같이 뽑을 수 있다.\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:   # 리스트 안에서 enumerate를 하면 튜플로 인덱스 0부터 묶이고,  딕셔너리 안에서 enumerate를 하면 인덱스와 값이 key, value로 들어간다.\n",
        "        index = token_index.get(word)      # word 부분이 token_index의 딕셔너리의 key로 들어가서 index에 value인 숫자가 들어간다.\n",
        "        results[i, j, index] = 1.          # i는 문장의 인덱스, j는 문장안에서 단어의 인덱스, index는 toekn_index의 value값\n",
        "        # print(index) \n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O3rvRGCGq0B",
        "outputId": "623ff430-e4a1-484a-8342-cba36ad2a4b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 케라스를 이용한 ohe\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The cat sat on the mat','The dog ate my homework.']\n",
        "\n",
        "tokenizer = Tokenizer(num_words=1000) # 가장 빈도가 높은 1000개의 단어만 선택하는 Tokenizer 객체 생성\n",
        "\n",
        "tokenizer.fit_on_texts(samples) # 단어 인덱스 구축\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(samples)  # 문자열을 정수인덱스의 리스트로 변환\n",
        "\n",
        "ohe_results = tokenizer.texts_to_matrix(samples,mode='binary')\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print(f'Found {len(word_index)} unique tokens')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mG98EzrD41-",
        "outputId": "d8815a28-0a1d-4216-9ff0-67ac710a0ea3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 unique tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y29g5iKFrvl",
        "outputId": "70ea4d01-4d3a-45aa-a285-c167dbd2b5ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEtK9wrGF2vW",
        "outputId": "558d1174-2379-49b2-f316-f9f62ffd6590"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding\n",
        "\n",
        "- 단어와 벡터를 연관짓는 강력하고 인기 있는 또 다른 방법은 단어 임베딩이라는 밀집 단어 벡터를 사용하는 것이다. \n",
        "- 원-핫 인코딩으로 만든 벡터는 희소하고(대부분 0으로 채워집니다) 고차원이다(어휘 사전에 있는 단어의 수와 차원이 같다). \n",
        "- 반면 단어 임베딩은 저차원의 실수형 벡터이다(희소 벡터의 반대인 밀집 벡터이다). 그림 6-2를 참고\n",
        "- 원-핫 인코딩으로 얻은 단어 벡터와 달리 단어 임베딩은 데이터로부터 학습된다. - 보통 256차원, 512차원 또는 큰 어휘 사전을 다룰 때는 1,024차원의 단어 임베딩을 사용한다.\n",
        "- 반면 원-핫 인코딩은 (20,000개의 토큰으로 이루어진 어휘 사전을 만들려면) 20,000차원 또는 그 이상의 벡터일 경우가 많다. \n",
        "- 따라서 단어 임베딩이 더 많은 정보를 적은 차원에 저장한다.\n",
        "* (문서 분류나 감성 예측과 같은) 관심 대상인 문제와 함께 단어 임베딩을 학습합니다. 이런 경우에는 랜덤한 단어 벡터로 시작해서 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습합니다.\n",
        "* 풀려는 문제가 아니고 다른 머신 러닝 작업에서 미리 계산된 단어 임베딩을 로드합니다. 이를 사전 훈련된 단어 임베딩이라고 합니다.\n",
        "- 실제 단어 임베딩 공간에서 의미 있는 기하학적 변환의 일반적인 예는 '성별' 벡터와 '복수(plural)' 벡터ㅇ이다. \n",
        "- 예를 들어 'king' 벡터에 'female' 벡터를 더하면 'queen' 벡터가 된다. 'plural' 벡터를 더하면 'kings'가 된다. \n",
        "- 단어 임베딩 공간은 전형적으로 이런 해석 가능하고 잠재적으로 유용한 수천 개의 벡터를 특성으로 가진다.\n",
        "- 사람의 언어를 완벽하게 매핑해서 어떤 자연어 처리 작업에도 사용할 수 있는 이상적인 단어 임베딩 공간은 아직 가능하지 않다. 사람의 언어에도 그런 것은 없다. \n",
        "- 세상에는 많은 다른 언어가 있고 언어는 특정 문화와 환경을 반영하기 때문에 서로 동일하지 않다. \n",
        "- 실제로 좋은 단어 임베딩 공간을 만드는 것은 문제에 따라 크게 달라진다. - 영어로 된 영화 리뷰 감성 분석 모델을 위한 완벽한 단어 임베딩 공간은 영어로 된 법률 문서 분류 모델을 위한 완벽한 임베딩 공간과 다를 것 이다. 특정 의미 관계의 중요성이 작업에 따라 다르기 때문이다.\n",
        "- 따라서 새로운 작업에는 새로운 임베딩을 학습하는 것이 타당하다. \n",
        "- 다행히 역전파를 사용해 쉽게 만들 수 있고 케라스를 사용하면 더 쉽다. `Embedding` 층의 가중치를 학습하면 된다.\n",
        "\n"
      ],
      "metadata": {
        "id": "OdBiCklNF4Ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(1000,64)"
      ],
      "metadata": {
        "id": "oWoaRc63GR3U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMDB 영화 감성 예측"
      ],
      "metadata": {
        "id": "2yxzu-PBLBS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "max_features = 10000 # 특성으로 사용할 단어의 수\n",
        "max_len = 20 # 사용할 텍스트의 길이(가장 빈번한 max_features 개의 단어만 사용)\n",
        "\n",
        "(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=max_len)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=max_len)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQZV9ZgnO1nZ",
        "outputId": "050fad20-6425-4b94-d97f-e405c2ad05e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U20wzJoJPkmC",
        "outputId": "5167a702-13d1-4153-e136-f3c87f5a6b49"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrV73wRyQA3T",
        "outputId": "6c497c76-a5c2-428e-a0fc-fb4ee92db581"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential \n",
        "from keras.layers import Flatten,Dense,Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(1000,8,input_length=max_len)) # 8은 embedding dimensionlity\n",
        "# 출력 크기는 (samples,maxlen,8)\n",
        "# 3D 임베딩 텐서를 (samples,maxlen*8) 크기의 2D 텐서로 펼침\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"acc\"])\n",
        "model.summary()\n",
        "model.fit(x_train,y_train,validation_split=0.2,epochs=10,batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdxEpTjcQMY5",
        "outputId": "7fd6de66-b1b5-4c0b-90c7-a3349318b48b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 20, 8)             8000      \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 160)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 161       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,161\n",
            "Trainable params: 8,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 6s 3ms/step - loss: 0.6734 - acc: 0.6112 - val_loss: 0.6283 - val_acc: 0.6894\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5652 - acc: 0.7253 - val_loss: 0.5486 - val_acc: 0.7204\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5074 - acc: 0.7495 - val_loss: 0.5312 - val_acc: 0.7292\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4880 - acc: 0.7588 - val_loss: 0.5282 - val_acc: 0.7332\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4775 - acc: 0.7657 - val_loss: 0.5284 - val_acc: 0.7338\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4696 - acc: 0.7721 - val_loss: 0.5293 - val_acc: 0.7380\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4624 - acc: 0.7775 - val_loss: 0.5315 - val_acc: 0.7366\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4552 - acc: 0.7824 - val_loss: 0.5353 - val_acc: 0.7350\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4478 - acc: 0.7879 - val_loss: 0.5376 - val_acc: 0.7340\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4406 - acc: 0.7925 - val_loss: 0.5408 - val_acc: 0.7344\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8f2d744070>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ojihwSCJRTEj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}