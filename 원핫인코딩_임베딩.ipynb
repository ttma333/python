{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1JyUZtWw35kJlemQE8S798Oc4KRi4sa3R",
      "authorship_tag": "ABX9TyMWzsYg6a1WlEH9Q00pgv0H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttma333/python/blob/main/%EC%9B%90%ED%95%AB%EC%9D%B8%EC%BD%94%EB%94%A9_%EC%9E%84%EB%B2%A0%EB%94%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLypXQfrDCS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8adc73b4-5684-42a9-fd70-7aa81c4a81c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/딥러닝 수업\n"
          ]
        }
      ],
      "source": [
        "cd/content/drive/MyDrive/딥러닝 수업"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 넘파이를 이용한 원-핫 인코딩\n",
        "import numpy as np\n",
        "\n",
        "samples = ['The cat sat on the mat.','The dog ate my homework']\n",
        "\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "    for word in sample.split(' '):\n",
        "        if word not in token_index:\n",
        "            token_index[word] = len(token_index) + 1 # 인덱스 0은 사용하지 않음\n",
        "\n",
        "max_length = 10 # 각 샘플에서 max_length 까지 단어만 사용\n",
        "results = np.zeros((len(samples),max_length,max(token_index.values())+1)) # 3차원 (2,10,11)\n",
        "\n",
        "for i,sample in enumerate(samples):\n",
        "    for j,word in list(enumerate(sample.split()))[:max_length]:\n",
        "        index = token_index.get(word)\n",
        "        results[i,j,index] = 1.\n",
        "print(results)\n",
        "\n",
        "# 원핫 인코딩의 형태\n",
        "token_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eThkH8SEZGvs",
        "outputId": "cb460f74-4b55-4842-8ee6-e60101d45bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'The': 1,\n",
              " 'cat': 2,\n",
              " 'sat': 3,\n",
              " 'on': 4,\n",
              " 'the': 5,\n",
              " 'mat.': 6,\n",
              " 'dog': 7,\n",
              " 'ate': 8,\n",
              " 'my': 9,\n",
              " 'homework': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy로 원핫인코딩\n",
        "import numpy as np\n",
        "\n",
        "samples = ['나는 오늘도 어제처럼 자연어처리 공부를 한다', '한파이와 함께라면 자연어처리 공부를 쉽게 한다']\n",
        "token_index = {}\n",
        "# print('numpy 확인')\n",
        "for sample in samples:\n",
        "    for word in sample.split(): # split로 띄어쓰기 기준으로 토큰을 나눈다\n",
        "        # print(word)\n",
        "        if word not in token_index:\n",
        "            token_index[word] = len(token_index) + 1     # 0 인덱스는 사용하지 않는다.\n",
        "# print(token_index)    # //{'나는': 1, '오늘도': 2, '어제처럼': 3, '자연어처리': 4, '공부를': 5, '한다': 6, '한파이와': 7, '함께라면': 8, '쉽게': 9}\n",
        "\n",
        "max_length = 10 # 각 샘플의 단어는 max_length까지만 허용한다. 즉, 여기서는 한 문장에 10개의 단어까지만 허용 할 수 있다는 이야기다.\n",
        "results = np.zeros(shape=(len(samples),\n",
        "                            max_length,\n",
        "                            max(token_index.values()) + 1))\n",
        "# print(token_index.values())      # dict_values([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "print(results)                   # 결과를 저장할 배열을 만든거다.\n",
        "\n",
        "for i, sample in enumerate(samples):     # enumerate는 인덱스부분을 i에서 같이 뽑을 수 있다.\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:   # 리스트 안에서 enumerate를 하면 튜플로 인덱스 0부터 묶이고,  딕셔너리 안에서 enumerate를 하면 인덱스와 값이 key, value로 들어간다.\n",
        "        index = token_index.get(word)      # word 부분이 token_index의 딕셔너리의 key로 들어가서 index에 value인 숫자가 들어간다.\n",
        "        results[i, j, index] = 1.          # i는 문장의 인덱스, j는 문장안에서 단어의 인덱스, index는 toekn_index의 value값\n",
        "        # print(index) \n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O3rvRGCGq0B",
        "outputId": "623ff430-e4a1-484a-8342-cba36ad2a4b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 케라스를 이용한 ohe\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The cat sat on the mat','The dog ate my homework.']\n",
        "\n",
        "tokenizer = Tokenizer(num_words=1000) # 가장 빈도가 높은 1000개의 단어만 선택하는 Tokenizer 객체 생성\n",
        "\n",
        "tokenizer.fit_on_texts(samples) # 단어 인덱스 구축\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(samples)  # 문자열을 정수인덱스의 리스트로 변환\n",
        "\n",
        "ohe_results = tokenizer.texts_to_matrix(samples,mode='binary')\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print(f'Found {len(word_index)} unique tokens')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mG98EzrD41-",
        "outputId": "d8815a28-0a1d-4216-9ff0-67ac710a0ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 unique tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y29g5iKFrvl",
        "outputId": "70ea4d01-4d3a-45aa-a285-c167dbd2b5ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEtK9wrGF2vW",
        "outputId": "558d1174-2379-49b2-f316-f9f62ffd6590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding\n",
        "\n",
        "- 단어와 벡터를 연관짓는 강력하고 인기 있는 또 다른 방법은 단어 임베딩이라는 밀집 단어 벡터를 사용하는 것이다. \n",
        "- 원-핫 인코딩으로 만든 벡터는 희소하고(대부분 0으로 채워집니다) 고차원이다(어휘 사전에 있는 단어의 수와 차원이 같다). \n",
        "- 반면 단어 임베딩은 저차원의 실수형 벡터이다(희소 벡터의 반대인 밀집 벡터이다). 그림 6-2를 참고\n",
        "- 원-핫 인코딩으로 얻은 단어 벡터와 달리 단어 임베딩은 데이터로부터 학습된다. - 보통 256차원, 512차원 또는 큰 어휘 사전을 다룰 때는 1,024차원의 단어 임베딩을 사용한다.\n",
        "- 반면 원-핫 인코딩은 (20,000개의 토큰으로 이루어진 어휘 사전을 만들려면) 20,000차원 또는 그 이상의 벡터일 경우가 많다. \n",
        "- 따라서 단어 임베딩이 더 많은 정보를 적은 차원에 저장한다.\n",
        "* (문서 분류나 감성 예측과 같은) 관심 대상인 문제와 함께 단어 임베딩을 학습합니다. 이런 경우에는 랜덤한 단어 벡터로 시작해서 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습합니다.\n",
        "* 풀려는 문제가 아니고 다른 머신 러닝 작업에서 미리 계산된 단어 임베딩을 로드합니다. 이를 사전 훈련된 단어 임베딩이라고 합니다.\n",
        "- 실제 단어 임베딩 공간에서 의미 있는 기하학적 변환의 일반적인 예는 '성별' 벡터와 '복수(plural)' 벡터ㅇ이다. \n",
        "- 예를 들어 'king' 벡터에 'female' 벡터를 더하면 'queen' 벡터가 된다. 'plural' 벡터를 더하면 'kings'가 된다. \n",
        "- 단어 임베딩 공간은 전형적으로 이런 해석 가능하고 잠재적으로 유용한 수천 개의 벡터를 특성으로 가진다.\n",
        "- 사람의 언어를 완벽하게 매핑해서 어떤 자연어 처리 작업에도 사용할 수 있는 이상적인 단어 임베딩 공간은 아직 가능하지 않다. 사람의 언어에도 그런 것은 없다. \n",
        "- 세상에는 많은 다른 언어가 있고 언어는 특정 문화와 환경을 반영하기 때문에 서로 동일하지 않다. \n",
        "- 실제로 좋은 단어 임베딩 공간을 만드는 것은 문제에 따라 크게 달라진다. - 영어로 된 영화 리뷰 감성 분석 모델을 위한 완벽한 단어 임베딩 공간은 영어로 된 법률 문서 분류 모델을 위한 완벽한 임베딩 공간과 다를 것 이다. 특정 의미 관계의 중요성이 작업에 따라 다르기 때문이다.\n",
        "- 따라서 새로운 작업에는 새로운 임베딩을 학습하는 것이 타당하다. \n",
        "- 다행히 역전파를 사용해 쉽게 만들 수 있고 케라스를 사용하면 더 쉽다. `Embedding` 층의 가중치를 학습하면 된다.\n",
        "\n"
      ],
      "metadata": {
        "id": "OdBiCklNF4Ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(1000,64)"
      ],
      "metadata": {
        "id": "oWoaRc63GR3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMDB 영화 감성 예측"
      ],
      "metadata": {
        "id": "2yxzu-PBLBS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "max_features = 10000 # 특성으로 사용할 단어의 수\n",
        "max_len = 20 # 사용할 텍스트의 길이(가장 빈번한 max_features 개의 단어만 사용)\n",
        "\n",
        "(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=max_len)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=max_len)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQZV9ZgnO1nZ",
        "outputId": "050fad20-6425-4b94-d97f-e405c2ad05e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U20wzJoJPkmC",
        "outputId": "5167a702-13d1-4153-e136-f3c87f5a6b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrV73wRyQA3T",
        "outputId": "6c497c76-a5c2-428e-a0fc-fb4ee92db581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential \n",
        "from keras.layers import Flatten,Dense,Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(1000,8,input_length=max_len)) # 8은 embedding dimensionlity\n",
        "# 출력 크기는 (samples,maxlen,8)\n",
        "# 3D 임베딩 텐서를 (samples,maxlen*8) 크기의 2D 텐서로 펼침\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"acc\"])\n",
        "model.summary()\n",
        "model.fit(x_train,y_train,validation_split=0.2,epochs=10,batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdxEpTjcQMY5",
        "outputId": "7fd6de66-b1b5-4c0b-90c7-a3349318b48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 20, 8)             8000      \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 160)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 161       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,161\n",
            "Trainable params: 8,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 6s 3ms/step - loss: 0.6734 - acc: 0.6112 - val_loss: 0.6283 - val_acc: 0.6894\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5652 - acc: 0.7253 - val_loss: 0.5486 - val_acc: 0.7204\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5074 - acc: 0.7495 - val_loss: 0.5312 - val_acc: 0.7292\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4880 - acc: 0.7588 - val_loss: 0.5282 - val_acc: 0.7332\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4775 - acc: 0.7657 - val_loss: 0.5284 - val_acc: 0.7338\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4696 - acc: 0.7721 - val_loss: 0.5293 - val_acc: 0.7380\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4624 - acc: 0.7775 - val_loss: 0.5315 - val_acc: 0.7366\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4552 - acc: 0.7824 - val_loss: 0.5353 - val_acc: 0.7350\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4478 - acc: 0.7879 - val_loss: 0.5376 - val_acc: 0.7340\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4406 - acc: 0.7925 - val_loss: 0.5408 - val_acc: 0.7344\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8f2d744070>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 원본 IMDB 텍스트 다운로드"
      ],
      "metadata": {
        "id": "ojihwSCJRTEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n"
      ],
      "metadata": {
        "id": "G2dkkG2rPRd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04c00818-2029-4c15-bf6f-0950bcbde36b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  28.0M      0  0:00:02  0:00:02 --:--:-- 28.0M\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd/content/drive/MyDrive/딥러닝 수업"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VxCONo3UCgD",
        "outputId": "e8e8ff1e-ccd3-481c-8b76-04bbc0310cbb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/딥러닝 수업\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요없어서 삭제\n",
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "2Ye-TNl-3Yvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d360bd58-85f0-4f0e-d547-8e923d11bf19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 긍정리뷰 읽어오기\n",
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYsdJxQr3mfC",
        "outputId": "d942bda5-1cd6-4eb3-f7f2-f6279647618e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "imdb_dir = './aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "  dir_name = os.path.join(train_dir, label_type)\n",
        "  for fname in os.listdir(dir_name):\n",
        "    if fname[-4:] == '.txt':\n",
        "      f = open(os.path.join(dir_name, fname), encoding='utf8')\n",
        "      texts.append(f.read())\n",
        "      f.close()\n",
        "      if label_type == 'neg':\n",
        "        labels.append(0)\n",
        "      else:\n",
        "        labels.append(1)\n"
      ],
      "metadata": {
        "id": "yL_BG0YI3mjJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "FJnaK5ReFWLP",
        "outputId": "1d1e563f-5149-40ce-cfa5-1b3447822adc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 토큰화\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100\n",
        "training_samples=15000\n",
        "validation_samples = 5000\n",
        "max_words = 10000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "len(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaGhxVYG5Khb",
        "outputId": "2f7ea8cb-7f2d-45cf-f150-894f0b969e06"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88582"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dbnOpEPZNZ6",
        "outputId": "ad23d073-5684-4bab-c6ab-d62d67e83527"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "data = pad_sequences(sequences,maxlen=maxlen)\n",
        "labels = np.asarray(labels)\n",
        "print(data.shape,labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggoZX-556GMr",
        "outputId": "a8fcd82b-bbd2-46f8-8587-ddc7641ea1a0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 100) (25000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련세트와 검증세트 분할\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples:training_samples+validation_samples]\n",
        "y_val = labels[training_samples:training_samples+validation_samples]\n",
        "x_test = data[training_samples+validation_samples:]\n",
        "y_test = labels[training_samples+validation_samples]"
      ],
      "metadata": {
        "id": "3Dv6jBAc6Wmh"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF8m8jTXG_Ew",
        "outputId": "eded9a8e-7a85-473f-a2f3-4f4b187528d4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sequences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYSlHMEcG_NG",
        "outputId": "071ee602-8b64-47c2-a319-15143a977837"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLMJgAroG_Ue",
        "outputId": "a24e2a9e-6e46-4abe-d57e-64392bb24553"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "285"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.shape)\n",
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEMS7IfnHOVX",
        "outputId": "1f0bbb0c-6d5d-4f50-b145-a3efeb34976f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 100)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  26,   13,  266,  358,   10, 1249,   89,    9,  249,   98,  159,\n",
              "        779,   39, 1117, 1187,  204,  123,  553,    9,   13,  176, 2424,\n",
              "          2,    9,  283,   85,  143,   46,   15, 5046,   24,  541,   40,\n",
              "          8,   58, 4363,    2,   10,   37, 4570, 1933,    8,   82,  351,\n",
              "       4128,    2,   11,    6, 2850,   18,   10,   25,  108, 1117, 4211,\n",
              "        365,    2,    8,  159,  779,  540,    2, 2035,  204, 1835,   50,\n",
              "        917,    2,  418,   50, 1802,    8,    1, 4728,    4,    1, 1117,\n",
              "       5347,  220,   31,  227,   71,   11,   17, 2422,    2,   12,   59,\n",
              "         27,   20,    3,  354,  311,   30,    1,  310,   48,    3,  384,\n",
              "        177], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_dir = '/content/drive/MyDrive/딥러닝 수업'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding='utf8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word]=coefs\n",
        "f.close()\n",
        "\n",
        "print(len(embeddings_index))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFwG9G4j6WsC",
        "outputId": "c6f47220-816f-4e52-b8ed-c24b768ac1b0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words,embedding_dim))\n",
        "for word,i in word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if i < max_words:\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "09G432IM6WuZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index.get('the')"
      ],
      "metadata": {
        "id": "JWsp6uGfA0Ii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb8a293-fa39-4448-d38d-a589969c8e4e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
              "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
              "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
              "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
              "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
              "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
              "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
              "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
              "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
              "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
              "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
              "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
              "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
              "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
              "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
              "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
              "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_matrix.shape)\n",
        "embedding_matrix[10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POfObEdXA954",
        "outputId": "c9d08e08-6360-4244-d3fb-3497c81397d0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 100)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.046539  ,  0.61966002,  0.56647003, -0.46584001, -1.18900001,\n",
              "        0.44599   ,  0.066035  ,  0.31909999,  0.14679   , -0.22119001,\n",
              "        0.79238999,  0.29905   ,  0.16073   ,  0.025324  ,  0.18678001,\n",
              "       -0.31000999, -0.28108001,  0.60514998, -1.0654    ,  0.52476001,\n",
              "        0.064152  ,  1.03579998, -0.40779001, -0.38011   ,  0.30801001,\n",
              "        0.59964001, -0.26991001, -0.76034999,  0.94221997, -0.46919   ,\n",
              "       -0.18278   ,  0.90652001,  0.79671001,  0.24824999,  0.25713   ,\n",
              "        0.6232    , -0.44768   ,  0.65357   ,  0.76902002, -0.51229   ,\n",
              "       -0.44332999, -0.21867   ,  0.38370001, -1.14830005, -0.94397998,\n",
              "       -0.15062   ,  0.30012   , -0.57805997,  0.20175   , -1.65910006,\n",
              "       -0.079195  ,  0.026423  ,  0.22051001,  0.99713999, -0.57538998,\n",
              "       -2.72659993,  0.31448001,  0.70521998,  1.43809998,  0.99125999,\n",
              "        0.13976   ,  1.34739995, -1.1753    ,  0.0039503 ,  1.02980006,\n",
              "        0.064637  ,  0.90886998,  0.82871997, -0.47003001, -0.10575   ,\n",
              "        0.5916    , -0.42210001,  0.57331002, -0.54114002,  0.10768   ,\n",
              "        0.39783999, -0.048744  ,  0.064596  , -0.61436999, -0.28600001,\n",
              "        0.50669998, -0.49757999, -0.81569999,  0.16407999, -1.96300006,\n",
              "       -0.26693001, -0.37593001, -0.95846999, -0.85839999, -0.71577001,\n",
              "       -0.32343   , -0.43121001,  0.41391999,  0.28374001, -0.70931   ,\n",
              "        0.15003   , -0.2154    , -0.37616   , -0.032502  ,  0.80620003])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,Flatten,Dense"
      ],
      "metadata": {
        "id": "vGV1KGLqA984"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.backend_config import epsilon\n",
        "from keras.layers.attention.multi_head_attention import activation\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words,embedding_dim,input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "model.layers[0].set_weights([embedding_matrix]) #모델이 Glove 임베딩 로드\n",
        "model.layers[0].trainable = False # 사전 훈련된 부분이 업데이트 되면 안됨\n",
        "\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
        "\n",
        "history = model.fit(x_train,y_train,epochs=10,batch_size=32,validation_data=(x_val,y_val))"
      ],
      "metadata": {
        "id": "HlmwnJMvA-Bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938b3571-27a4-4afa-92b3-b6ea61736fdb"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 10000)             0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 32)                320032    \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,320,065\n",
            "Trainable params: 1,320,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.7010 - acc: 0.5032 - val_loss: 0.6933 - val_acc: 0.4886\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.6936 - acc: 0.5053 - val_loss: 0.6933 - val_acc: 0.4896\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.6924 - acc: 0.5066 - val_loss: 0.6933 - val_acc: 0.4884\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.6920 - acc: 0.5077 - val_loss: 0.6935 - val_acc: 0.4886\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.6917 - acc: 0.5103 - val_loss: 0.6935 - val_acc: 0.4884\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.6901 - acc: 0.5133 - val_loss: 0.6937 - val_acc: 0.4878\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.6884 - acc: 0.5165 - val_loss: 0.6939 - val_acc: 0.4924\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.6849 - acc: 0.5213 - val_loss: 0.7027 - val_acc: 0.4878\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.6744 - acc: 0.5402 - val_loss: 0.7072 - val_acc: 0.5110\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.6463 - acc: 0.6031 - val_loss: 0.7257 - val_acc: 0.5038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJN8YzANYBAC",
        "outputId": "b9d6b511-a8dc-49f4-bfe0-80933d661318"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 과제 샘플 2000개로 하고 돌리기"
      ],
      "metadata": {
        "id": "7oUcOCnFS09s"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fm2z3ub9X4aG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}